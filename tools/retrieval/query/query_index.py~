import multiprocessing
import time
import sys

import joblib

sys.path.append("/home/boxinw-src/megatron-lm/megatron")
sys.path.append("/home/boxinw-src/megatron-lm/")

import h5py
import numpy as np
from tqdm import tqdm

import os, psutil

import argparse


parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('--index-path', type=str, default='',
                    help='path to load the faiss index')
parser.add_argument('--feature-path', type=str, default='',
                    help='path to load the chunk feature hdf5')
parser.add_argument('--doc-path', type=str, default='',
                    help='path to load the banned doc id pkl')
parser.add_argument('--chunk-path', type=str, default='',
                    help='path to load the chunk doc id pkl')
# parser.add_argument('--out-path', type=str, default='',
#                     help='data path to load the jsonl')
parser.add_argument('--out-dir', type=str, required=True,
                    help='output dir.')
parser.add_argument('--out-prefix', type=str, required=True,
                    help='output prefix.')
parser.add_argument('--start', type=int, default=0,
                   help='Number of worker processes to launch')
parser.add_argument('--offset', type=int, default=0,
                   help='Number of worker processes to launch')
parser.add_argument('--split', type=int, default=2,
                   help='Number of splits of input features')
parser.add_argument('--target-k', type=int, default=200,
                   help='Number of neighbors to dump')
parser.add_argument('--k', type=int, default=2000,
                   help='Number of neighbors to retrieve')
parser.add_argument('--workers', type=int, default=20,
                   help='Number of worker processes to launch')
# faiss index
parser.add_argument('--efsearch', type=int, default=256,
                   help='Number of worker processes to launch')
parser.add_argument('--nprobe', type=int, default=65536,
                   help='Number of worker processes to launch')
parser.add_argument("--zfs", action='store_true', default=False,
                   help='Use zfs data.')

args = parser.parse_args()

process = psutil.Process(os.getpid())
print(process.memory_info().rss / 1024 / 1024 / 1024)  # in bytes

import faiss                   # make faiss available
ngpus = faiss.get_num_gpus()

print("number of GPUs:", ngpus)

root1 = "/gpfs/fs1/projects/gpu_adlr/datasets/boxinw/processed_data/chunks/"
root2 = "/home/dcg-adlr-boxinw-data/processed_data/chunks/"

print(args.index_path)
# index database
start = time.time()
index = faiss.read_index(args.index_path)
end = time.time()
print("Loading data time cost:", end - start)

## load banned document id list
# doc_ids = joblib.load('/gpfs/fs1/projects/gpu_adlr/datasets/boxinw/pretrained_data/Wikipedia-shuf/Wikipedia_en_ftfy_id_shuf_text_document.doc_ids.pkl')
doc_ids = joblib.load(args.doc_path)

## load chunk id
f = h5py.File(args.chunk_path, "r")
document_ids = np.copy(f['document_id'])
f.close()




from faiss import ParameterSpace

ParameterSpace().set_index_parameter(index, "efSearch", args.efsearch)
ParameterSpace().set_index_parameter(index, "nprobe", args.nprobe)

print("efSearch", args.efsearch)
print("nprobe", args.nprobe)

# for data_start in range(args.split):
#     args.start = data_start
for data_start in range(args.start, args.split):
    print("Loading features...")
    # features to be queried
    f = h5py.File(args.feature_path, "r")
    features = f['feat']
    length = len(features)
    split_size = int(np.ceil(length / args.split))
    # >>>
    # start = split_size * args.start
    # end = min(split_size * (args.start + 1), length)
    # +++
    start = split_size * data_start
    end = min(split_size * (data_start + 1), length)
    # <<<
    print(f"Start Index: {start}, End Index: {end}, Split: {args.split}, Split size: {split_size}")
    data = np.copy(features[start:end])
    f.close()

    print("features dim:", features.shape)

    ## query
    print("Searching kNN...")
    start = time.time()
    k = args.k                             # we want to see 2000 nearest neighbors
    D, I = index.search(data, k)         # sanity check
    end = time.time()
    print("time cost:", end - start)

    neighbors = np.zeros((len(data), args.target_k), 'uint64')

    tot = 0
    start = time.time()

    def filter_neighbors(i):
        chunk_id = i + data_start * split_size
        sample_id = chunk_id // 32 + args.offset
        neighbors = I[i]
        banned_docs = doc_ids[sample_id]

        filtered_neighbors = []

        for neighbor in neighbors:
            if document_ids[neighbor] not in banned_docs:
                filtered_neighbors.append(neighbor)
            if len(filtered_neighbors) == args.target_k:
                return filtered_neighbors
        return filtered_neighbors

    pool = multiprocessing.Pool(args.workers)

    neighbor_ind = np.arange(len(data))
    delayed_neighbors = pool.imap(filter_neighbors, neighbor_ind, 25)

    for i, filtered_neighbors in enumerate(tqdm(delayed_neighbors, total=len(neighbor_ind))):
        if len(filtered_neighbors) < args.target_k:
            filtered_neighbors += [-1] * (args.target_k - len(filtered_neighbors))
            tot += 1
        assert len(filtered_neighbors) == args.target_k
        neighbors[i] = filtered_neighbors
    end = time.time()
    print("Number of neighbors < target k:", tot)
    print("time cost:", end - start)

    # for i in tqdm(range(len(data))):
    #     filtered_neighbors = filter_neighbors(i)
    #     if len(filtered_neighbors) < args.target_k:
    #         filtered_neighbors += [-1] * (args.target_k - len(filtered_neighbors))
    #         tot += 1
    #     assert len(filtered_neighbors) == args.target_k
    #     neighbors[i] = filtered_neighbors
    #
    # print("Number of neighbors < target k:", tot)

    # if not args.out_path:
    #     out_path = f"{args.feature_path}_neighbors_start_{args.start}_split_{args.split}.hdf5"
    # else:
    #     out_path = args.out_path
    # out_path = f"{args.out_dir}/{args.out_prefix}__neighbors_start_{args.start}_split_{args.split}.hdf5"
    out_path = f"{args.out_dir}/{args.out_prefix}__neighbors_start_{data_start}_split_{args.split}.hdf5"
    print(f"Dumping to {out_path}")

    fout = h5py.File(out_path, "w")
    fout.create_dataset("neighbors", data=neighbors)
    fout.close()
