# lawrence mcafee

# ~~~~~~~~ import ~~~~~~~~
import argparse
import faiss
# import glob
# import h5py
# import numpy as np
import os
import time

from lutil import pax

from ..data.load_train_data import load_train_data
from .init_index import init_index
from ..util.get_index_paths import get_index_paths
from ..util.move_index_to_gpu import move_index_to_gpu

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def run_train_pipeline(args):

    time_map = {}

    # ~~~~~~~~ split feat files ~~~~~~~~
    # if 0:
    #     split_feat_files()
    #     raise Exception("split feat files.")

    # ~~~~~~~~ load train data ~~~~~~~~
    start_time = time.time()
    train_data = load_train_data(args) # .ntrain)
    time_map["load"] = time.time() - start_time # load_feat_time
    print("time / load data : %s." % time_map["load"], flush = True)

    # ~~~~~~~~ index paths ~~~~~~~~
    get_index_paths(args)

    assert not os.path.isfile(args.trained_index_path), \
        "index exists: '%s'." % args.trained_index_path

    # ~~~~~~~~ init index ~~~~~~~~
    start_time = time.time()
    index = init_index(args.index_str, train_data.shape[1])
    index = move_index_to_gpu(index)
    time_map["init"] = time.time() - start_time # init_index_time
    print("time / init index : %s." % time_map["init"], flush = True)

    # ~~~~~~~~ train index ~~~~~~~~
    start_time = time.time()
    index.train(train_data) # , verbose = True)
    time_map["train"] = time.time() - start_time # train_index_time
    print("time / train index : %s." % time_map["train"], flush = True)

    # ~~~~~~~~ save index ~~~~~~~~
    start_time = time.time()
    faiss.write_index(index, args.trained_index_path)
    time_map["save"] = time.time() - start_time
    print("time / save index : %s." % time_map["save"], flush = True)

    # ~~~~~~~~ debug ~~~~~~~~
    # pax({"time_map": time_map})

    # ~~~~~~~~ return ~~~~~~~~
    return time_map

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def run_add_pipeline(args):

    time_map = {}

    # ~~~~~~~~ load data ~~~~~~~~
    start_time = time.time()
    train_data = load_train_data(args) # .ntrain)
    time_map["load-data"] = time.time() - start_time # load_feat_time
    print("time / load data : %s." % time_map["load-data"], flush = True)

    # ~~~~~~~~ index paths ~~~~~~~~
    get_index_paths(args)

    assert not os.path.isfile(args.added_index_path), \
        "index exists: '%s'." % args.added_index_path

    # ~~~~~~~~ load trained index ~~~~~~~~
    start_time = time.time()
    index = faiss.read_index(args.trained_index_path)
    time_map["load-index"] = time.time() - start_time
    print("time / load index : %s." % time_map["load-index"], flush = True)

    # ~~~~~~~~ move index to gpu ~~~~~~~~
    start_time = time.time()
    index = move_index_to_gpu(index)
    time_map["transfer"] = time.time() - start_time
    print("time / transfer index : %s." % time_map["transfer"], flush = True)

    # >>> [ redundant ]
    faiss.ParameterSpace().set_index_parameter(index, "verbose", 1)
    # index.verbose = True # ... maybe?
    # <<<

    # pax({"index": index})

    # ~~~~~~~~ add vectors ~~~~~~~~
    start_time = time.time()
    batch_size = int(1e6)
    for i0 in range(0, len(train_data), batch_size):
        i1 = min(len(train_data), i0 + batch_size)
        print(
            "index.add %d:%d [ of %d ]." % (i0, i1, len(train_data)),
            flush = True,
        )
        index.add(train_data[i0:i1])
    time_map["add"] = time.time() - start_time # add_index_time
    print("time / add index : %s." % time_map["add"], flush = True)

    # ~~~~~~~~ save index ~~~~~~~~
    start_time = time.time()
    faiss.write_index(index, args.added_index_path)
    time_map["save"] = time.time() - start_time
    print("time / save index : %s." % time_map["save"], flush = True)

    # ~~~~~~~~ debug ~~~~~~~~
    # pax({"time_map": time_map})

    # ~~~~~~~~ return ~~~~~~~~
    return time_map

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if __name__ == "__main__":

    print("hi, index.", flush = True)

    # >>>
    # print(dict(os.environ), flush = True)
    # print("pythonpath = %s." % os.environ["PYTHONPATH"], flush = True)
    # exit(0)

    # print("\n>> train = %s. <<\n" % str(train.shape), flush = True)
    # pax({"num gpus": faiss.get_num_gpus()})
    # <<<

    # ~~~~~~~~ user args ~~~~~~~~
    parser = argparse.ArgumentParser()
    parser.add_argument("--task", required = True,
                        choices = [ "train", "add", "query" ])
    parser.add_argument("--ntrain", "-t", type = int, required = True)
    # parser.add_argument("--ncluster", "-c", type = int, required = True)
    # parser.add_argument("--index-ty", "-i", required = True,
    #                     choices = [ "basic", "fancy" ])
    parser.add_argument("--index-str", "-i", required = True)
    args = parser.parse_args()
    
    # pax({"args": args})

    # # ~~~~~~~~ internal args ~~~~~~~~
    # from types import SimpleNamespace
    # args = SimpleNamespace()

    args.base_dir = "/gpfs/fs1/projects/gpu_adlr/datasets/lmcafee/retrieval"
    # args.task = cargs.task

    # # args.ntrain = int(1e5)
    # # args.ntrain = int(1e4)
    # # args.ntrain =  int(5e6)
    # args.ntrain = cargs.ntrain

    # # args.ncluster = 2**13 # 8192
    # # args.ncluster = 2**14 # 16384
    # # args.ncluster = 2**18 # 262144 [ basic ]
    # # args.ncluster = 2**22 # 4194304 [ fancy ]
    # args.ncluster = cargs.ncluster

    # args.index_str = cargs.index_str

    # pax({"args": args})

    # ~~~~~~~~ pipeline ~~~~~~~~
    if args.task == "train":
        time_map = run_train_pipeline(args)
    elif args.task == "add":
        time_map = run_add_pipeline(args)
    elif args.task == "query":
        raise Exception("hi.")
        time_map = run_query_pipeline(args)
    else:
        raise Exception("specialize for task '%s'." % args.task)

    # ~~~~~~~~ stats ~~~~~~~~
    # print("train %s ... time %.1f; [ load %.1f, init %.1f, train %.1f, add %.1f ] ... %s." % (
    print("t %d, c %d ... time %.1f; [ %s ] ... %s." % (
        # train_data.shape,
        args.ntrain,
        args.ncluster,
        # load_feat_time,
        # init_index_time,
        # train_index_time,
        # add_index_time,
        sum(time_map.values()),
        ", ".join("%s %.1f" % (k, v) for k, v in time_map.items()),
        args.index_str,
    ), flush = True)

# eof
