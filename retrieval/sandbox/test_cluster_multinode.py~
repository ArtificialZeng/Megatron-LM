# lawrence mcafee

# ~~~~~~~~ import ~~~~~~~~
import argparse
import faiss
import numpy as np

from lutil import pax

from lawrence.utils import Timer

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def run_faiss(args, data, timer):

    # ~~~~~~~~ pipeline ~~~~~~~~
    timer.push("init")
    ivf = faiss.IndexIVFFlat(
        faiss.IndexFlatL2(args.nfeats),
        args.nfeats,
        args.ncenters,
    )
    timer.pop()

    timer.push("gpu")
    clustering_index = faiss.index_cpu_to_all_gpus(faiss.IndexFlatL2(ivf.d))
    ivf.clustering_index = clustering_index
    timer.pop()

    ivf.verbose = True
    ivf.quantizer.verbose = True
    ivf.clustering_index.verbose = True
    
    timer.push("train")
    ivf.train(data)
    timer.pop()

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def run_cuml(args, data, timer):

    from cuml.cluster import KMeans
    # from cuml.datasets import make_blobs

    timer.push("init")
    k_means = KMeans(
        n_clusters = args.ncenters,
        verbose = True,
        max_iter = 30,
        init = "scalable-k-means++",
        # init = "k-means||",
        # init = "random",
    )
    timer.pop()

    # raise Exception("initialized.")

    timer.push("fit")
    k_means.fit(data)
    timer.pop()

    raise Exception("trained.")

    # timer.push("predict")
    # labels = k_means.predict(data)
    # timer.pop()

    pax({"labels": labels})

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
def run_dask(args, data, timer):

    from cuml.dask.cluster import KMeans
    from cuml.dask.datasets import make_blobs
    from dask.distributed import Client
    import dask.array as da
    from dask_cuda import LocalCUDACluster

    timer.push("client")
    # c = Client() # <scheduler_address>)
    cluster = LocalCUDACluster()
    client = Client(cluster)
    timer.pop()

    # data, _ = make_blobs(n_samples = 1000, n_features = 1024, centers = 100)
    data = da.from_array(data)
    # pax({"data": data})

    timer.push("init")
    # k_means = KMeans(n_clusters = args.ncenters, verbose = True, max_iters = 30)
    k_means = KMeans(
        n_clusters = args.ncenters,
        # verbose = True,
        max_iter = 30,
        init = "scalable-k-means++",
        # init = "k-means||",
        # init = "random",
    )
    timer.pop()

    # raise Exception("initialized.")

    timer.push("fit")
    k_means.fit(data)
    timer.pop()

    timer.print()
    raise Exception("trained.")

    # timer.push("predict")
    # labels = k_means.predict(data)
    # timer.pop()

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if __name__ == "__main__":

    # ~~~~~~~~ user args ~~~~~~~~
    parser = argparse.ArgumentParser()
    parser.add_argument("--ncenters", required = True)
    parser.add_argument("--ntrain", required = True)
    parser.add_argument("--nfeats", default = 1024)
    args = parser.parse_args()

    args.ncenters = int(float(args.ncenters))
    args.ntrain = int(float(args.ntrain))
    args.ngpus = faiss.get_num_gpus()

    # pax({"args": args})

    # ~~~~~~~~ timer ~~~~~~~~
    timer = Timer()

    # ~~~~~~~~ data ~~~~~~~~
    timer.push("data")
    data = np.random.rand(args.ntrain, args.nfeats).astype("f4")
    timer.pop()

    # pax({"data": str(data.shape)})

    # run_faiss(args, data, timer)
    # run_cuml(args, data, timer)
    run_dask(args, data, timer)

    # ~~~~~~~~ stats ~~~~~~~~
    print("~~~~~~~~~~~~~~~~")
    timer.print()
    print("~~~~~~~~~~~~~~~~")
    # pax({"args": args})
    print("L-RESULT : t %d, c %d ... %s." % (
        args.ntrain,
        args.ncenters,
        timer.get_child_str(None),
    ), flush = True)

# eof
